====================================================================================================
反向精度测试 (PyTorch autograd vs Triton kernel)
====================================================================================================
/home/users/chenquanlin/temp.py:1036: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  max_diff = abs_diff.max().item()

Name         Config                                                  MaxDiff      RelDiff(%)   Pass  
-------------------------------------------------------------------------------------------------
小规模          batch=1, heads=16, chunk=256, seq=512, dim=576, topk=128 1.49e-08     2.95e-05     ✓     
中等规模         batch=1, heads=64, chunk=512, seq=1024, dim=576, topk=256 1.49e-08     2.54e-05     ✓     
大规模          batch=1, heads=128, chunk=1024, seq=2048, dim=576, topk=512 1.49e-08     2.22e-05     ✓     
多batch       batch=2, heads=64, chunk=256, seq=512, dim=576, topk=128 1.12e-01     4.78e+01     ✗     
大topk        batch=1, heads=128, chunk=512, seq=2048, dim=576, topk=1024 7.45e-09     1.92e-05     ✓     
-------------------------------------------------------------------------------------------------
反向测试: 4/5 通过
