// AttnDist Kernel: Compute attention distribution using pre-computed LSE
// This kernel computes attn_sum = sum(exp2(qk * sm_scale - lse), dim=heads) for each topk position

#include "fwd.h"

#include <math_constants.h>
#include <cute/tensor.hpp>
#include <cutlass/cluster_launch.hpp>
#include <cooperative_groups.h>
#include <cutlass/arch/reg_reconfig.h>
#include <cutlass/arch/arch.h>

#include "utils.h"
#include "helpers.h"

namespace sm90 {

using namespace cute;

constexpr int D_Q = 576;
constexpr int D_K = 576;

constexpr int B_H = 64;         // Head block size
constexpr int B_TOPK = 64;      // TopK block size
constexpr int NUM_THREADS = 128 * 2;  // Simplified: 2 warpgroups (consumer + producer)

template<int NUM_TILES>
using SmemLayoutQTiles = decltype(coalesce(tile_to_shape(
    GMMA::Layout_K_SW128_Atom<bf16>{},
    Shape<Int<B_H>, Int<64*NUM_TILES>>{},
    Step<_1, _2>{}
), Shape<_1, _1>{}));

template<int NUM_TILES>
using SmemLayoutKTiles = decltype(coalesce(tile_to_shape(
    GMMA::Layout_SW128_Atom<bf16, GMMA::Major::K>{},
    Shape<Int<B_TOPK>, Int<64*NUM_TILES>>{},
    Step<_1, _2>{}
), Shape<_1, _1>{}));

using SmemLayoutQ = SmemLayoutQTiles<9>;  // 9 tiles for D_Q=576
using SmemLayoutK = SmemLayoutKTiles<9>;

using TiledMMA_QK = decltype(make_tiled_mma(
    GMMA::MMA_64x64x16_F32BF16BF16_SS<GMMA::Major::K, GMMA::Major::K>{},
    Layout<Shape<_1, _1, _1>>{}
));

// Shared memory plan for attn_dist kernel
struct AttnDistSharedMemory {
    array_aligned<bf16, cosize_v<SmemLayoutQ>> q;
    array_aligned<bf16, cosize_v<SmemLayoutK>> k[2];  // Double buffer for K
    
    bool is_k_valid[2][B_TOPK];
    float lse[B_H];  // Pre-computed LSE for current position
    float attn_sum_partial[B_TOPK];  // Partial sums for reduction
    
    transac_bar_t bar_q;
    transac_bar_t bar_k_free[2], bar_k_ready[2];
    transac_bar_t bar_is_k_valid_ready;
};

template<typename TmaParams>
struct AttnDistTmaParams {
    typename TmaParams::Shape_Q shape_Q;
    typename TmaParams::TMA_Q tma_Q;
};

template<typename Shape_Q, typename TMA_Q>
struct TmaParamsAttnDist {
    Shape_Q shape_Q;
    TMA_Q tma_Q;
};

enum AttnDistBarriers : uint32_t {
    attn_dist_warpgroup_sync = 0,
    attn_dist_reduction_sync = 1
};

template<typename TmaParams>
__global__ void __launch_bounds__(NUM_THREADS, 1, 1)
attn_dist_kernel(__grid_constant__ const AttnDistParams params, __grid_constant__ const TmaParams tma_params) {
#if IS_SM90
    const int q_h_idx = blockIdx.x % (params.h_q / B_H);  // Which head block
    const int s_q_idx = blockIdx.x / (params.h_q / B_H);  // Which sequence position
    const int warpgroup_idx = cutlass::canonical_warp_group_idx();
    const int warp_idx = cutlass::canonical_warp_idx_sync();
    const int idx_in_warpgroup = threadIdx.x % 128;

    // Shared memory
    extern __shared__ char wksp_buf[];
    AttnDistSharedMemory &smem = *reinterpret_cast<AttnDistSharedMemory*>(wksp_buf);
    Tensor sQ = make_tensor(make_smem_ptr(smem.q.data()), SmemLayoutQ{});

    // Initialize barriers
    if (warp_idx == 0 && elect_one_sync()) {
        cute::prefetch_tma_descriptor(tma_params.tma_Q.get_tma_descriptor());
        
        smem.bar_q.init(1);
        CUTE_UNROLL
        for (int i = 0; i < 2; ++i) {
            smem.bar_k_free[i].init(128);
            smem.bar_k_ready[i].init(128);
        }
        smem.bar_is_k_valid_ready.init(16);
        fence_barrier_init();
    }
    __syncthreads();

    // Calculate causal mask limit
    const int q_pos = params.q_start_index_s + s_q_idx;
    const int max_k_idx = (q_pos + 1 - params.k_stride) / params.k_stride;

    const int num_topk_blocks = params.topk / B_TOPK;

    if (warpgroup_idx == 0) {
        // Consumer warpgroup: compute QK^T and reduce
        cutlass::arch::warpgroup_reg_alloc<216>();

        // Load Q via TMA
        if (warp_idx == 0 && elect_one_sync()) {
            Tensor gQ = flat_divide(
                tma_params.tma_Q.get_tma_tensor(tma_params.shape_Q)(_, _, s_q_idx),
                Tile<Int<B_H>, Int<D_Q>>{}
            )(_, _, q_h_idx, _0{});
            launch_tma_copy(tma_params.tma_Q, gQ, sQ, smem.bar_q, TMA::CacheHintSm90::EVICT_FIRST);
            smem.bar_q.arrive_and_expect_tx(B_H * D_Q * sizeof(bf16));
        }

        // Load LSE from global memory (cooperatively)
        {
            const int lse_offset = s_q_idx * params.h_q + q_h_idx * B_H;
            const int threads_per_load = 128;
            const int elems_per_thread = (B_H + threads_per_load - 1) / threads_per_load;
            for (int i = 0; i < elems_per_thread; ++i) {
                int idx = idx_in_warpgroup + i * threads_per_load;
                if (idx < B_H) {
                    smem.lse[idx] = params.lse[lse_offset + idx];
                }
            }
        }
        __syncwarp();

        // Allocate registers for accumulation
        Tensor rP = partition_fragment_C(TiledMMA_QK{}, Shape<Int<B_H>, Int<B_TOPK>>{});

        // Wait for Q
        smem.bar_q.wait(0);

        bool cur_bar_wait_phase = 0;

        // Process each topk block
        CUTE_NO_UNROLL
        for (int block_idx = 0; block_idx < num_topk_blocks; ++block_idx) {
            // Wait for K to be ready
            smem.bar_k_ready[block_idx % 2].wait(cur_bar_wait_phase);
            smem.bar_is_k_valid_ready.wait(cur_bar_wait_phase);

            Tensor sK = make_tensor(make_smem_ptr(smem.k[block_idx % 2].data()), SmemLayoutK{});

            // Compute QK^T via GEMM
            TiledMMA tiled_mma_QK = TiledMMA_QK{};
            
            // Process all D_K/64 = 9 tiles
            CUTE_UNROLL
            for (int tile_idx = 0; tile_idx < 9; ++tile_idx) {
                Tensor sQ_tile = flat_divide(sQ, Tile<Int<B_H>, Int<64>>{})(_, _, _0{}, tile_idx);
                Tensor sK_tile = make_tensor(make_smem_ptr(smem.k[block_idx % 2].data() + tile_idx * B_TOPK * 64), SmemLayoutKTiles<1>{});
                gemm_ss(tile_idx == 0, tiled_mma_QK, sQ_tile, sK_tile, rP, idx_in_warpgroup);
            }
            warpgroup_commit_batch();
            warpgroup_wait<0>();

            // Apply exp2(qk * sm_scale - lse) and mask invalid K positions
            const float scale = params.sm_scale_div_log2;
            
            // Each thread processes its elements in rP
            // rP layout: 2 rows per thread, multiple columns
            CUTE_UNROLL
            for (int row_idx = 0; row_idx < 2; ++row_idx) {
                int real_row = get_AorC_row_idx(row_idx, idx_in_warpgroup);
                float lse_val = smem.lse[real_row];
                
                CUTE_UNROLL
                for (int i = row_idx * 2; i < size(rP); i += 4) {
                    int col0 = get_AorC_col_idx(i, idx_in_warpgroup);
                    int col1 = get_AorC_col_idx(i + 1, idx_in_warpgroup);
                    
                    bool valid0 = smem.is_k_valid[block_idx % 2][col0];
                    bool valid1 = smem.is_k_valid[block_idx % 2][col1];
                    
                    // Apply softmax with pre-computed LSE
                    rP(i) = valid0 ? exp2f(rP(i) * scale - lse_val) : 0.0f;
                    rP(i + 1) = valid1 ? exp2f(rP(i + 1) * scale - lse_val) : 0.0f;
                }
            }

            // Reduce sum across heads for each topk position
            // First, each thread computes partial sums for its columns
            // Then we need to reduce across all threads that handle the same column
            
            // Initialize partial sums in shared memory
            if (idx_in_warpgroup < B_TOPK) {
                smem.attn_sum_partial[idx_in_warpgroup] = 0.0f;
            }
            __syncwarp();
            
            // Each thread accumulates its contribution
            // Thread layout: each thread holds 2 rows x multiple cols in rP
            // We need to sum across all rows (heads) for each column (topk position)
            CUTE_UNROLL
            for (int row_idx = 0; row_idx < 2; ++row_idx) {
                CUTE_UNROLL
                for (int i = row_idx * 2; i < size(rP); i += 4) {
                    int col0 = get_AorC_col_idx(i, idx_in_warpgroup);
                    int col1 = get_AorC_col_idx(i + 1, idx_in_warpgroup);
                    
                    atomicAdd(&smem.attn_sum_partial[col0], rP(i));
                    atomicAdd(&smem.attn_sum_partial[col1], rP(i + 1));
                }
            }
            
            // Sync before writing to global memory
            NamedBarrier::arrive_and_wait(128, AttnDistBarriers::attn_dist_warpgroup_sync);
            
            // Write attn_sum to global memory
            const int output_offset = s_q_idx * params.topk + block_idx * B_TOPK;
            if (idx_in_warpgroup < B_TOPK) {
                params.attn_sum[output_offset + idx_in_warpgroup] = smem.attn_sum_partial[idx_in_warpgroup];
            }
            
            // Signal K buffer is free
            smem.bar_k_free[block_idx % 2].arrive();
            
            if ((block_idx % 2) == 1) {
                cur_bar_wait_phase ^= 1;
            }
        }
    } else {
        // Producer warpgroup: load K from global memory
        cutlass::arch::warpgroup_reg_dealloc<72>();

        constexpr int GROUP_SIZE = 8, NUM_GROUPS = 128 / GROUP_SIZE;
        constexpr int NUM_ROWS_PER_GROUP = B_TOPK / NUM_GROUPS;
        int idx_in_group = idx_in_warpgroup % GROUP_SIZE;
        int group_idx = idx_in_warpgroup / GROUP_SIZE;
        int* gIndices = params.indices + s_q_idx * params.topk;

        bf16* my_sK_base = &(make_tensor(make_smem_ptr(smem.k[0].data()), SmemLayoutKTiles<1>{})(group_idx, idx_in_group * 8));
        bf16* my_gK_base = params.k + idx_in_group * 8;

        int64_t token_indices[NUM_ROWS_PER_GROUP];
        bool is_token_valid[NUM_ROWS_PER_GROUP];

        int64_t cache_policy = createpolicy_evict_last();

        int cur_bar_wait_phase = 1;

        CUTE_NO_UNROLL
        for (int block_idx = 0; block_idx < num_topk_blocks; ++block_idx) {
            int buf_idx = block_idx % 2;

            // Load token indices and check validity
            CUTE_UNROLL
            for (int local_row = 0; local_row < NUM_ROWS_PER_GROUP; ++local_row) {
                int offs = block_idx * B_TOPK + local_row * NUM_GROUPS + group_idx;
                int t = __ldg(gIndices + offs);
                token_indices[local_row] = t * (int64_t)params.stride_k_s_kv;
                // Check both index validity and causal mask
                is_token_valid[local_row] = (t >= 0) && (t < params.s_kv) && (t <= max_k_idx);
            }

            // Wait for buffer to be free
            smem.bar_k_free[buf_idx].wait(cur_bar_wait_phase);

            // Copy K tiles from global to shared memory
            CUTE_UNROLL
            for (int local_row = 0; local_row < NUM_ROWS_PER_GROUP; ++local_row) {
                int64_t token_index = token_indices[local_row];
                bool valid = is_token_valid[local_row];
                CUTE_UNROLL
                for (int tile_idx = 0; tile_idx < 9; ++tile_idx) {  // 9 tiles for D_K=576
                    cp_async_cacheglobal_l2_prefetch_256B(
                        my_gK_base + token_index + tile_idx * 64,
                        my_sK_base + (buf_idx * B_TOPK * D_K + tile_idx * (B_TOPK * 64) + local_row * NUM_GROUPS * 64),
                        valid,
                        cache_policy
                    );
                }
            }
            cutlass::arch::cpasync_barrier_arrive_noinc((uint64_t*)(&smem.bar_k_ready[buf_idx]));

            // Update validity mask
            if (idx_in_group == 0) {
                CUTE_UNROLL
                for (int local_row = 0; local_row < NUM_ROWS_PER_GROUP; ++local_row) {
                    smem.is_k_valid[buf_idx][local_row * NUM_GROUPS + group_idx] = is_token_valid[local_row];
                }
                smem.bar_is_k_valid_ready.arrive();
            }

            if ((block_idx % 2) == 1) {
                cur_bar_wait_phase ^= 1;
            }
        }
    }
#else
    if (cute::thread0()) {
        CUTE_INVALID_CONTROL_PATH("This kernel only supports sm90");
    }
#endif
}


void run_attn_dist_kernel(const AttnDistParams& params) {
    FLASH_ASSERT(params.h_kv == 1);
    FLASH_ASSERT(params.topk % B_TOPK == 0);
    FLASH_ASSERT(params.topk > 0);
    FLASH_ASSERT(params.h_q % B_H == 0);
    FLASH_ASSERT(params.d_qk == D_Q);

    auto shape_Q = make_shape(params.h_q, params.d_qk, params.s_q);
    auto tma_Q = cute::make_tma_copy(
        SM90_TMA_LOAD{},
        make_tensor(
            make_gmem_ptr((bf16*)params.q),
            make_layout(
                shape_Q,
                make_stride(params.stride_q_h_q, _1{}, params.stride_q_s_q)
            )
        ),
        SmemLayoutQ{}
    );

    TmaParamsAttnDist<decltype(shape_Q), decltype(tma_Q)> tma_params = {
        shape_Q, tma_Q
    };
    auto kernel = &attn_dist_kernel<decltype(tma_params)>;

    constexpr size_t smem_size = sizeof(AttnDistSharedMemory);
    CHECK_CUDA(cudaFuncSetAttribute(kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size));

    cutlass::ClusterLaunchParams launch_params = {
        dim3((params.h_q / B_H) * params.s_q, 1, 1),
        dim3(NUM_THREADS, 1, 1),
        dim3(1, 1, 1),
        smem_size,
        params.stream
    };
    cutlass::launch_kernel_on_cluster(
        launch_params, (void*)kernel, params, tma_params
    );
    CHECK_CUDA_KERNEL_LAUNCH();
}

}  // namespace sm90
