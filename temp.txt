template<int D_QK>
void run_bwd_phase1_kernel(const SparseAttnBwdParams& params) {
    static_assert(D_QK == 576 || D_QK == 512);  // Only support these two QK dimensions
    using Kernel = KernelTemplate<D_QK>;

    // === Parameter validation ===
    KU_ASSERT(params.h_kv == 1);                    // KV head count must be 1 (MLA feature)
    KU_ASSERT(params.topk % Kernel::B_TOPK == 0);   // TopK must be multiple of B_TOPK (skip boundary check)
    KU_ASSERT(params.h_q == Kernel::B_H);           // Query head count must equal B_H
    KU_ASSERT(params.d_qk == D_QK);
    KU_ASSERT(params.delta != nullptr);             // Delta output must be provided
    KU_ASSERT(params.stride_delta_s_q > 0);         // Delta stride must be positive
    KU_ASSERT(params.stride_delta_h_q > 0);         // Delta stride must be positive
    
    // === Additional debug validations ===
    KU_ASSERT(params.s_q > 0);                      // s_q must be positive for valid grid
    KU_ASSERT(params.s_kv > 0);                     // s_kv must be positive
    KU_ASSERT(params.topk > 0);                     // topk must be positive
    KU_ASSERT(params.q != nullptr);                 // Q tensor must be valid
    KU_ASSERT(params.kv != nullptr);                // KV tensor must be valid
    KU_ASSERT(params.dO != nullptr);                // dO tensor must be valid
    KU_ASSERT(params.o != nullptr);                 // O tensor must be valid (for delta computation)
    KU_ASSERT(params.indices != nullptr);           // indices must be valid
    KU_ASSERT(params.d_v == Kernel::D_V);           // D_V must match (512)
    
    // === Debug: Print shared memory size ===
    constexpr size_t smem_size_check = sizeof(typename Kernel::SharedMemoryPlan);
    // SM100 max dynamic shared memory is typically 228KB (233472 bytes)
    KU_ASSERT(smem_size_check <= 233472 && "SharedMemoryPlan exceeds SM100 max dynamic shared memory");

    // === Create TMA descriptor for Q (2SM mode) ===
    // Q shape: [h_q, d_qk, s_q] = [h_q, 576, s_q] (包含 NoPE 512 + RoPE 64)
    auto shape_Q = make_shape(params.h_q, Kernel::D_Q, params.s_q);
    auto tma_Q = cute::make_tma_copy(
        SM100_TMA_2SM_LOAD_NOSPLIT{},  // 2SM TMA for cluster
        make_tensor(
            make_gmem_ptr((bf16*)params.q),
            make_layout(
                shape_Q,
                make_stride(params.stride_q_h_q, _1{}, params.stride_q_s_q)
            )
        ),
        (typename Kernel::SmemLayoutQ){}
    );

    // === Create TMA descriptor for dO (2SM mode) ===
    // dO shape: [h_q, d_v, s_q]
    auto shape_dO = make_shape(params.h_q, params.d_v, params.s_q);
    auto tma_dO = cute::make_tma_copy(
        SM100_TMA_2SM_LOAD_NOSPLIT{},  // 2SM TMA for cluster
        make_tensor(
            make_gmem_ptr((bf16*)params.dO),
            make_layout(
                shape_dO,
                make_stride(params.stride_dO_h_q, _1{}, params.stride_dO_s_q)
            )
        ),
        (typename Kernel::SmemLayoutdO){}
    );

    // === Create TensorMap for KV (for TMA gather) ===
    // Note: O is read directly from global memory in Delta computation, no TMA needed
    // KV shape: [d_qk, s_kv] = [576, s_kv] (包含 NoPE 512 + RoPE 64)
    CUtensorMap tensor_map_kv;
    {
        uint64_t size[2] = {Kernel::D_Q, (unsigned long)params.s_kv};  // [d_qk, s_kv]
        uint64_t stride[1] = {params.stride_kv_s_kv*sizeof(bf16)};  // Row stride
        uint32_t box_size[2] = {64, 1};    // Box size per load: 64 cols x 1 row
        uint32_t elem_stride[2] = {1, 1};  // Element stride
        CUresult res = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
            &tensor_map_kv,
            CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_BFLOAT16,
            2,                                                      // 2D tensor
            params.kv,                                              // Global memory pointer
            size,
            stride,
            box_size,
            elem_stride,
            CUtensorMapInterleave::CU_TENSOR_MAP_INTERLEAVE_NONE,   // No interleave
            CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_128B,         // 128-byte swizzle
            CUtensorMapL2promotion::CU_TENSOR_MAP_L2_PROMOTION_L2_256B,  // L2 cache promotion
            CUtensorMapFloatOOBfill::CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE   // No OOB fill
        );
        KU_ASSERT(res == CUresult::CUDA_SUCCESS);
    }

    // === Assemble TMA parameter struct ===
    TmaParams<
        decltype(shape_Q), decltype(tma_Q),
        decltype(shape_dO), decltype(tma_dO)
    > tma_params = {
        shape_Q, tma_Q,
        shape_dO, tma_dO,
        tensor_map_kv
    };
    
    // 使用 KernelTemplate 实例化的 kernel (与正向传播保持一致)
    auto kernel = &sparse_attn_bwd_kernel_part0<Kernel, decltype(tma_params)>;

    // === Configure and launch kernel with cluster (2CTA mode) ===
    constexpr size_t smem_size = sizeof(typename Kernel::SharedMemoryPlan);  // Dynamic shared memory size
    
    // Debug: Verify shared memory size is within limits
    int device_id;
    KU_CUDA_CHECK(cudaGetDevice(&device_id));
    cudaDeviceProp prop;
    KU_CUDA_CHECK(cudaGetDeviceProperties(&prop, device_id));
    KU_ASSERT(smem_size <= (size_t)prop.sharedMemPerBlockOptin && 
              "SharedMemoryPlan exceeds device max dynamic shared memory");
    
    // Debug: Verify cluster support
    KU_ASSERT(prop.major >= 9 && "Cluster launch requires SM90+ (compute capability 9.0+)");
    
    KU_CUDA_CHECK(cudaFuncSetAttribute(kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size));
    
    // Debug: Verify grid dimensions
    KU_ASSERT(2*params.s_q <= prop.maxGridSize[0] && "Grid dimension X exceeds device limit");
    KU_ASSERT(Kernel::NUM_THREADS <= prop.maxThreadsPerBlock && "Block size exceeds device limit");
    
    // Critical: Grid size must be a multiple of cluster size
    KU_ASSERT((2*params.s_q) % 2 == 0 && "Grid X dimension must be multiple of cluster X dimension (2)");
    // Grid X = 2*s_q should always be even, but verify anyway

    // 2CTA Cluster Launch:
    // - Grid: 2*s_q blocks (2 CTAs per query token)
    // - Block: NUM_THREADS (384) threads
    // - Cluster: (2, 1, 1) - 2 CTAs form a cluster
    cutlass::ClusterLaunchParams launch_params = {
        dim3(2*params.s_q, 1, 1),       // Grid: 2 CTAs per query
        dim3(Kernel::NUM_THREADS, 1, 1),// Block: 384 threads
        dim3(2, 1, 1),                  // Cluster: 2 CTAs
        smem_size,                      // Dynamic shared memory
        params.stream                   // CUDA stream
    };
    
    // Debug: Check maximum active blocks per SM
    int max_active_blocks = 0;
    cudaError_t occ_err = cudaOccupancyMaxActiveBlocksPerMultiprocessor(
        &max_active_blocks, kernel, Kernel::NUM_THREADS, smem_size
    );
    if (occ_err != cudaSuccess) {
        printf("Warning: cudaOccupancyMaxActiveBlocksPerMultiprocessor failed: %s\n", 
               cudaGetErrorString(occ_err));
    } else {
        KU_ASSERT(max_active_blocks >= 1 && "Kernel cannot be launched: max_active_blocks < 1");
    }
    
    // Debug: Get kernel function attributes
    cudaFuncAttributes func_attrs;
    cudaError_t attr_err = cudaFuncGetAttributes(&func_attrs, kernel);
    if (attr_err == cudaSuccess) {
        printf("=== Kernel Function Attributes ===\n");
        printf("  numRegs: %d\n", func_attrs.numRegs);
        printf("  sharedSizeBytes: %zu\n", func_attrs.sharedSizeBytes);
        printf("  constSizeBytes: %zu\n", func_attrs.constSizeBytes);
        printf("  localSizeBytes: %zu\n", func_attrs.localSizeBytes);
        printf("  maxThreadsPerBlock: %d\n", func_attrs.maxThreadsPerBlock);
        printf("  ptxVersion: %d\n", func_attrs.ptxVersion);
        printf("  binaryVersion: %d\n", func_attrs.binaryVersion);
        printf("  maxDynamicSharedSizeBytes: %d\n", func_attrs.maxDynamicSharedSizeBytes);
        printf("  preferredShmemCarveout: %d\n", func_attrs.preferredShmemCarveout);
        printf("  clusterDimMustBeSet: %d\n", func_attrs.clusterDimMustBeSet);
        printf("  requiredClusterWidth: %d\n", func_attrs.requiredClusterWidth);
        printf("  requiredClusterHeight: %d\n", func_attrs.requiredClusterHeight);
        printf("  requiredClusterDepth: %d\n", func_attrs.requiredClusterDepth);
        printf("  clusterSchedulingPolicyPreference: %d\n", func_attrs.clusterSchedulingPolicyPreference);
        printf("  nonPortableClusterSizeAllowed: %d\n", func_attrs.nonPortableClusterSizeAllowed);
        printf("==================================\n");
        
        // Validate registers: SM100 has 64K registers per SM
        // With 384 threads and max_active_blocks=1, we can use up to 65536/384 = ~170 regs per thread
        int max_regs_per_thread = 65536 / (Kernel::NUM_THREADS * max_active_blocks);
        printf("  Max regs per thread (estimated): %d\n", max_regs_per_thread);
        if (func_attrs.numRegs > max_regs_per_thread) {
            printf("  WARNING: Register usage (%d) may exceed limit (%d)!\n", 
                   func_attrs.numRegs, max_regs_per_thread);
        }
    } else {
        printf("Warning: cudaFuncGetAttributes failed: %s\n", cudaGetErrorString(attr_err));
    }
    
    // Debug: Check if cluster dimensions are specified in kernel
    // If clusterDimMustBeSet is true, we need to use cudaLaunchKernelEx
    if (attr_err == cudaSuccess && func_attrs.clusterDimMustBeSet) {
        printf("Note: Kernel requires cluster dimensions to be set explicitly\n");
        printf("  requiredClusterWidth: %d\n", func_attrs.requiredClusterWidth);
        printf("  requiredClusterHeight: %d\n", func_attrs.requiredClusterHeight);
        printf("  requiredClusterDepth: %d\n", func_attrs.requiredClusterDepth);
    }
    
    // Debug: Set cluster scheduling policy
    cudaError_t cluster_sched_err = cudaFuncSetAttribute(
        kernel, cudaFuncAttributeClusterSchedulingPolicyPreference, 
        cudaClusterSchedulingPolicySpread
    );
    if (cluster_sched_err != cudaSuccess) {
        printf("Warning: Failed to set cluster scheduling policy: %s\n", 
               cudaGetErrorString(cluster_sched_err));
    }
    
    // Try to set non-portable cluster size
    cudaError_t cluster_size_err = cudaFuncSetAttribute(
        kernel, cudaFuncAttributeNonPortableClusterSizeAllowed, 1
    );
    if (cluster_size_err != cudaSuccess) {
        printf("Warning: Failed to set non-portable cluster size: %s\n", 
               cudaGetErrorString(cluster_size_err));
    }
    
    // Debug: Check cluster size is supported on device
    int cluster_supported = 0;
    cudaError_t cluster_query_err = cudaDeviceGetAttribute(
        &cluster_supported, cudaDevAttrClusterLaunch, device_id
    );
    if (cluster_query_err == cudaSuccess) {
        printf("Device cluster launch support: %s\n", cluster_supported ? "YES" : "NO");
        if (!cluster_supported) {
            printf("ERROR: Device does not support cluster launch!\n");
        }
    } else {
        printf("Warning: Failed to query cluster support: %s\n", cudaGetErrorString(cluster_query_err));
    }
    
    // Debug: Synchronize and check for any prior CUDA errors
    cudaError_t sync_err = cudaDeviceSynchronize();
    if (sync_err != cudaSuccess) {
        printf("CUDA error before kernel launch: %s\n", cudaGetErrorString(sync_err));
    }
    cudaError_t prior_err = cudaGetLastError();
    if (prior_err != cudaSuccess) {
        printf("Prior CUDA error: %s\n", cudaGetErrorString(prior_err));
    }
    
    // Debug: Check memory alignment (TMA requires 128-byte alignment)
    constexpr size_t TMA_ALIGNMENT = 128;
    bool q_aligned = (reinterpret_cast<uintptr_t>(params.q) % TMA_ALIGNMENT == 0);
    bool kv_aligned = (reinterpret_cast<uintptr_t>(params.kv) % TMA_ALIGNMENT == 0);
    bool dO_aligned = (reinterpret_cast<uintptr_t>(params.dO) % TMA_ALIGNMENT == 0);
    bool o_aligned = (reinterpret_cast<uintptr_t>(params.o) % TMA_ALIGNMENT == 0);
    
    if (!q_aligned || !kv_aligned || !dO_aligned || !o_aligned) {
        printf("WARNING: Memory alignment issues detected!\n");
        printf("  q alignment: %s (addr: %p, offset: %zu)\n", 
               q_aligned ? "OK" : "FAIL", params.q, 
               reinterpret_cast<uintptr_t>(params.q) % TMA_ALIGNMENT);
        printf("  kv alignment: %s (addr: %p, offset: %zu)\n", 
               kv_aligned ? "OK" : "FAIL", params.kv,
               reinterpret_cast<uintptr_t>(params.kv) % TMA_ALIGNMENT);
        printf("  dO alignment: %s (addr: %p, offset: %zu)\n", 
               dO_aligned ? "OK" : "FAIL", params.dO,
               reinterpret_cast<uintptr_t>(params.dO) % TMA_ALIGNMENT);
        printf("  o alignment: %s (addr: %p, offset: %zu)\n", 
               o_aligned ? "OK" : "FAIL", params.o,
               reinterpret_cast<uintptr_t>(params.o) % TMA_ALIGNMENT);
    }
    
    // Debug: Check TmaParams struct size and alignment
    constexpr size_t tma_params_size = sizeof(decltype(tma_params));
    constexpr size_t tma_params_align = alignof(decltype(tma_params));
    printf("  TmaParams size: %zu bytes, alignment: %zu bytes\n", tma_params_size, tma_params_align);
    
    // Debug: Print launch configuration before launch
    printf("=== BWD Phase1 Launch Config ===\n");
    printf("  smem_size: %zu bytes (max: %zu)\n", smem_size, (size_t)prop.sharedMemPerBlockOptin);
    printf("  grid: (%d, 1, 1)\n", 2*params.s_q);
    printf("  block: (%d, 1, 1)\n", Kernel::NUM_THREADS);
    printf("  cluster: (2, 1, 1)\n");
    printf("  max_active_blocks: %d\n", max_active_blocks);
    printf("  device: %s (SM %d.%d)\n", prop.name, prop.major, prop.minor);
    printf("  params: s_q=%d, s_kv=%d, topk=%d, h_q=%d, d_qk=%d, d_v=%d\n", 
           params.s_q, params.s_kv, params.topk, params.h_q, params.d_qk, params.d_v);
    printf("  TMA descriptor addresses: q=%p, kv=%p, dO=%p, o=%p\n",
           params.q, params.kv, params.dO, params.o);
    printf("  Strides: q_s_q=%d, q_h_q=%d, kv_s_kv=%d\n",
           params.stride_q_s_q, params.stride_q_h_q, params.stride_kv_s_kv);
    printf("================================\n");
    
    // Debug option: Try direct CUDA launch first to isolate cluster vs kernel issues
    // Set to true to bypass cluster launch and use direct kernel launch for debugging
    constexpr bool DEBUG_USE_DIRECT_LAUNCH = false;
    
    cutlass::Status status;
    if constexpr (DEBUG_USE_DIRECT_LAUNCH) {
        // Direct launch without cluster (for debugging)
        // Note: This will break 2CTA features but helps isolate if issue is cluster-related
        printf("DEBUG: Using direct kernel launch (cluster features will be broken)\n");
        kernel<<<params.s_q, Kernel::NUM_THREADS, smem_size, params.stream>>>(params, tma_params);
        cudaError_t launch_err = cudaGetLastError();
        if (launch_err != cudaSuccess) {
            printf("Direct launch failed: %s\n", cudaGetErrorString(launch_err));
            status = cutlass::Status::kErrorInternal;
        } else {
            status = cutlass::Status::kSuccess;
        }
    } else {
        status = cutlass::launch_kernel_on_cluster(
            launch_params, (void*)kernel, params, tma_params
        );
    }
    
    // Debug: More detailed error reporting
    if (status != cutlass::Status::kSuccess) {
        printf("CUTLASS cluster launch failed with status: %d\n", (int)status);
        printf("  Error details:\n");
        printf("    Status 0 = kSuccess\n");
        printf("    Status 1 = kErrorMisalignedOperand\n");
        printf("    Status 2 = kErrorInvalidDataType\n");
        printf("    Status 3 = kErrorInvalidLayout\n");
        printf("    Status 4 = kErrorInvalidProblem\n");
        printf("    Status 5 = kErrorNotSupported\n");
        printf("    Status 6 = kErrorWorkspaceNull\n");
        printf("    Status 7 = kErrorCudaHostAlloc\n");
        printf("    Status 8 = kErrorInvalidWorkspace\n");
        printf("    Status 9 = kErrorInvalidKernel\n");
        printf("    Status 10 = kErrorArchMismatch\n");
        printf("    Status 11 = kErrorInternal\n");
        printf("    Status 12 = kErrorInvalidNode\n");
        printf("    Status 13 = kErrorMmaPolicyMismatch\n");
        
        // Check for any pending CUDA errors
        cudaError_t cuda_err = cudaGetLastError();
        if (cuda_err != cudaSuccess) {
            printf("  CUDA error: %s\n", cudaGetErrorString(cuda_err));
        }
    }
    KU_CUTLASS_CHECK(status);
}

}  // namespace sm100::bwd::head128
